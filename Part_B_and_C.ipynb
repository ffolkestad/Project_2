{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B and C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This notebook consist of the own Feed Fordward Neural Network. This code dervies from the weekly exercise from week 42, and lecture notes from week 38, 39, 40 and 41.\"\n",
    "\n",
    "\n",
    "# Imports the necessary libraries\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining activations functions, and their derivate. \n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_der(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "def ReLU_der(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def leaky_ReLU(z, alpha=0.01):\n",
    "    return np.where(z >= 0, z, alpha * z)\n",
    "\n",
    "def leaky_ReLU_der(z, alpha=0.01):\n",
    "    return np.where(z >= 0, 1, alpha)\n",
    "\n",
    "# Defining the MSE\n",
    "def mse(predict, target):\n",
    "    return np.mean((predict - target) ** 2)\n",
    "\n",
    "# Defining the R2 score\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model)**2) / np.sum((y_data - np.mean(y_data))**2)  #Lecture notes week 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layers for batched input\n",
    "def create_layers_batch(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    # Initialize input size for the first layer\n",
    "    i_size = network_input_size\n",
    "    # Iterate through each specified layer output size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        # Initialize weights (W) with small random values and biases (b) with random values\n",
    "        W = np.random.randn(i_size, layer_output_size)\n",
    "        #W = np.random.randn(i_size, layer_output_size) * np.sqrt(2 / (i_size + layer_output_size)) #Xavier-initialising\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        #b = np.zeros(layer_output_size)\n",
    "        # Append the layer (weights and biases) to the layers list\n",
    "        layers.append((W, b))\n",
    "\n",
    "        # Update the input size for the next layer\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "# Create Feed-forward function for batched input\n",
    "def feed_forward_batch(inputs, layers, activation_funcs):\n",
    "    a = inputs \n",
    "    # Iterate through each layer and activation function\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = np.dot(a, W) + b\n",
    "        a = activation_func(z) # Applying the activation functions\n",
    "    return a\n",
    "\n",
    "# Calculate the cost using Mean Squared Error (MSE)\n",
    "def cost_function(layers, inputs, activation_funcs, target):\n",
    "    predict = feed_forward_batch(inputs, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "# Saves the values from the feed-forward function. To be used in the backpropagation. \n",
    "def feed_forward_saver_batch(input_batch, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = input_batch\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = np.dot(a, W) + b\n",
    "        a = activation_func(z)\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a\n",
    "\n",
    "\n",
    "# Backpropagation for batched input\n",
    "def backpropagation_batch(\n",
    "    input_batch, layers, activation_funcs, targets, activation_ders):\n",
    "    layer_inputs, zs, predict = feed_forward_saver_batch(input_batch, layers, activation_funcs)\n",
    "    \n",
    "    batch_size = input_batch.shape[0]  # Size of the batch\n",
    "    layer_grads = []  # List to store the gradients in\n",
    "\n",
    "    error = predict - targets # Iniital differance between predictions and targets\n",
    "\n",
    "    # Iterate backwards through each layer\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z = layer_inputs[i], zs[i]\n",
    "\n",
    "        # Applies the derivate of the activation function except for the last layer\n",
    "        if i !=len(layers)-1: #To ecxlude the last layer\n",
    "            error = error * activation_ders[i](z)\n",
    "\n",
    "        # Calculates the gradients for the weights and biases\n",
    "        dW = np.dot(layer_input.T, error)/ batch_size\n",
    "        db = np.mean(error, axis=0)\n",
    "\n",
    "        # Inserts the gradients\n",
    "        layer_grads.insert(0, (dW,db))\n",
    "\n",
    "        # Calculate the error for the next layer\n",
    "        if i > 0:\n",
    "            error = np.dot(error, layers[i][0].T)\n",
    "\n",
    "    return layer_grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The Adam optimzer presented the best result in part A, and is therefore used in this section.\"\n",
    "\n",
    "# Function to train the network using Adam optimizer\n",
    "def train_network_adam(inputs, layers, activation_funcs, targets, activation_ders, learning_rate=0.001, epochs=20, batch_size=16):\n",
    "    n = len(inputs)  # Number of training samples\n",
    "\n",
    "    # Adam parameters that are set manually\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    delta = 1e-8\n",
    "\n",
    "    # Initialize moments for Adam\n",
    "    first_moment_W = [np.zeros_like(W) for W, b in layers]\n",
    "    second_moment_W = [np.zeros_like(W) for W, b in layers]\n",
    "    first_moment_b = [np.zeros_like(b) for W, b in layers]\n",
    "    second_moment_b = [np.zeros_like(b) for W, b in layers]\n",
    "\n",
    "    # Iterate through epochs\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n):\n",
    "            random_index = np.random.randint(n)\n",
    "            batch_inputs = inputs[random_index:random_index + batch_size]\n",
    "            batch_targets = targets[random_index:random_index + batch_size]\n",
    "\n",
    "            # Calculate the gradients using backpropagation\n",
    "            layer_grads = backpropagation_batch(batch_inputs, layers, activation_funcs, batch_targets, activation_ders)\n",
    "\n",
    "\n",
    "            # Update the weights and biases for each layer using Adam optimizer\n",
    "            for j, ((W, b), (dW, db)) in enumerate(zip(layers, layer_grads)):\n",
    "                # Update first moment estimate for weights\n",
    "                first_moment_W[j] = beta1 * first_moment_W[j] + (1 - beta1) * dW\n",
    "                second_moment_W[j] = beta2 * second_moment_W[j] + (1 - beta2) * (dW ** 2)\n",
    "\n",
    "                # Correct bias in first and second moment estimates for weights\n",
    "                m_hat_w = first_moment_W[j] / (1 - beta1 ** (epoch + 1))\n",
    "                v_hat_w = second_moment_W[j] / (1 - beta2 ** (epoch + 1))\n",
    "\n",
    "                # Update the weights\n",
    "                W_update = learning_rate * m_hat_w / (delta + np.sqrt(v_hat_w))\n",
    "                layers[j] = (W - W_update, b)\n",
    "\n",
    "                # Update the first moment estimate for biases\n",
    "                first_moment_b[j] = beta1 * first_moment_b[j] + (1 - beta1) * db\n",
    "                second_moment_b[j] = beta2 * second_moment_b[j] + (1 - beta2) * (db ** 2)\n",
    "\n",
    "                # Correct the bias in first and second moment estimates for biases\n",
    "                m_hat_b = first_moment_b[j] / (1 - beta1 ** (epoch + 1))\n",
    "                v_hat_b = second_moment_b[j] / (1 - beta2 ** (epoch + 1))\n",
    "\n",
    "                # Update the biases\n",
    "                b_update = learning_rate * m_hat_b / (delta + np.sqrt(v_hat_b))\n",
    "                layers[j] = (layers[j][0], b - b_update)\n",
    "\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions using the trained network\n",
    "def predict(inputs, layers, activation_funcs):\n",
    "    return feed_forward_batch(inputs, layers, activation_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data\n",
    "x_train = np.linspace(-10,10,100).reshape(-1, 1)\n",
    "y_train = (2.0 + 5 * x_train + 0.1 * x_train**2).reshape(-1,1)\n",
    "\n",
    "\n",
    "# Scales the data using StandardScaler from Scikit-learn\n",
    "scaler_y = StandardScaler()\n",
    "scaler_x= StandardScaler()\n",
    "\n",
    "# Standardiser x_train og y_train\n",
    "x_train_scaled = scaler_x.fit_transform(x_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Splits the dataset into test and training\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train_scaled, y_train_scaled, test_size=0.2, random_state=20)\n",
    "\n",
    "\n",
    "# Defines the in- and output-size of the network\n",
    "network_input_size = 1 \n",
    "layer_output_sizes = [10,10, 1] #Two hidden layers with 10 nodes. \n",
    "\n",
    "# Creates the layers with the specified structure\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:4.6478\n",
      "R2:0.9949\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(20)\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "# Sets the sigmoid funciton for the hidden layers\n",
    "\n",
    "activation_funcs = [sigmoid, sigmoid, lambda x:x]\n",
    "activation_ders = [sigmoid_der, sigmoid_der, lambda x: np.ones_like(x)]\n",
    "\n",
    "# Train the network, defines learning rate, batch size and epochs. \n",
    "train_network_adam(x_train_split, layers, activation_funcs, y_train_split, activation_ders, learning_rate=0.001,epochs=100, batch_size=16)\n",
    "\n",
    "# Makes predictions using the trained network \n",
    "pred_scaled = predict(x_test_split, layers, activation_funcs)\n",
    "# Invers the predicitons to make them the original scale\n",
    "pred = scaler_y.inverse_transform(pred_scaled)\n",
    "# Inverse the train split data to make them the original scale\n",
    "y_test_inverse = scaler_y.inverse_transform(y_test_split)\n",
    "\n",
    "#Calculates the MSE for the sigmoid function\n",
    "mse_sig= mse(y_test_inverse, pred)\n",
    "print( f\"MSE:{mse_sig:.4f}\")\n",
    "r2_sig = R2(y_test_inverse,pred)\n",
    "print( f\"R2:{r2_sig:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:1.4478\n",
      "R2:0.9984\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(20)\n",
    "# Re-initialize the network layers before testing with ReLU activation function\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "# Sets the ReLU funciton for the hidden layers\n",
    "activation_funcs = [ReLU, ReLU, lambda x:x]\n",
    "activation_ders = [ReLU_der, ReLU_der, lambda x: np.ones_like(x)]\n",
    "\n",
    "# Train the network, defines learning rate, batch size and epochs. \n",
    "train_network_adam(x_train_split, layers, activation_funcs, y_train_split, activation_ders, learning_rate=0.001,epochs=100, batch_size=16)\n",
    "\n",
    "# Makes predictions using the trained network \n",
    "pred_scaled = predict(x_test_split, layers, activation_funcs)\n",
    "# Inverstransforms the predicitons to make them the original scale\n",
    "pred = scaler_y.inverse_transform(pred_scaled)\n",
    "# Inverse the train split data to make them the original scale\n",
    "y_test_inverse = scaler_y.inverse_transform(y_test_split)\n",
    "\n",
    "#Calculates the MSE for the ReLU function\n",
    "mse_ReLU= mse(y_test_inverse, pred)\n",
    "print( f\"MSE:{mse_ReLU:.4f}\")\n",
    "\n",
    "r2_ReLU = R2(y_test_inverse, pred)\n",
    "print( f\"R2:{r2_ReLU:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky-ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:2.0979\n",
      "R2:0.9977\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(20)\n",
    "\n",
    "# Re-initialize the network layers before testing with ReLU activation function\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "# Sets the leaky-ReLU funciton for the hidden layers\n",
    "activation_funcs = [leaky_ReLU, leaky_ReLU, lambda x:x]\n",
    "activation_ders = [leaky_ReLU_der, leaky_ReLU_der, lambda x: np.ones_like(x)]\n",
    "\n",
    "# Train the network, defines learning rate, batch size and epochs. \n",
    "train_network_adam(x_train_split, layers, activation_funcs, y_train_split, activation_ders, learning_rate=0.001,epochs=100, batch_size=16)\n",
    "\n",
    "# Makes predictions using the trained network \n",
    "pred_scaled = predict(x_test_split, layers, activation_funcs)\n",
    "# Inverstransforms the predicitons to make them the original scale\n",
    "pred = scaler_y.inverse_transform(pred_scaled)\n",
    "# Inverse the train split data to make them the original scale\n",
    "y_test_inverse = scaler_y.inverse_transform(y_test_split)\n",
    "\n",
    "#Calculates the MSE for the sigmoid function\n",
    "mse_leakyReLU= mse(y_test_inverse, pred)\n",
    "print( f\"MSE:{mse_leakyReLU:.4f}\")\n",
    "\n",
    "r2_LeakyReLU = R2(y_test_inverse, pred)\n",
    "print( f\"R2:{r2_LeakyReLU:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results with MLP-regressor from Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data\n",
    "x_train = np.linspace(-10,10,100).reshape(-1, 1)\n",
    "y_train = (2.0 + 5 * x_train + 0.1 * x_train**2).reshape(-1,1)\n",
    "\n",
    "\n",
    "# Scales the data using StandardScaler from Scikit-learn\n",
    "scaler_y = StandardScaler()\n",
    "scaler_x= StandardScaler()\n",
    "\n",
    "# Standardiser x_train og y_train\n",
    "x_train_scaled = scaler_x.fit_transform(x_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train_scaled, y_train_scaled, test_size=0.2, random_state=20)\n",
    "\n",
    "\n",
    "# Defines the in- and output-size of the network\n",
    "network_input_size = 1 \n",
    "layer_output_sizes = [10,10, 1] #Two hidden layers with 10 nodes. \n",
    "\n",
    "# Creates the layers with the specified structure\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using Scikit-learn:4.4887\n",
      "MSE own NN using Sigmoid activation function:4.6478\n",
      "R² using Scikit-learn: 0.9950\n",
      "R² own NN using Sigmoid activation function: 0.9949\n"
     ]
    }
   ],
   "source": [
    "# Comparing with mlp-regressor from Skicit-learn \n",
    "np.random.seed(20)\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(10,10), activation=\"logistic\", solver=\"adam\", learning_rate_init=0.001, max_iter=500, tol=1e-6, random_state=20, early_stopping=True)\n",
    "mlp_regressor.fit(x_train_split, y_train_split.ravel())\n",
    "mlp_pred_scaled = mlp_regressor.predict(x_test_split).reshape(-1,1)\n",
    "mlp_pred = scaler_y.inverse_transform(mlp_pred_scaled)\n",
    "y_test_inverse = scaler_y.inverse_transform(y_test_split)\n",
    "\n",
    "mse_mlp = mse(y_test_inverse, mlp_pred)\n",
    "print( f\"MSE using Scikit-learn:{mse_mlp:.4f}\")\n",
    "print( f\"MSE own NN using Sigmoid activation function:{mse_sig:.4f}\")\n",
    "\n",
    "r2_mlp = R2(y_test_inverse, mlp_pred)\n",
    "print(f\"R² using Scikit-learn: {r2_mlp:.4f}\")\n",
    "print(f\"R² own NN using Sigmoid activation function: {r2_sig:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using Scikit-learn:3.4037\n",
      "MSE own NN using ReLU activation function:1.4478\n",
      "R² using Scikit-learn: 0.9962\n",
      "R² own NN using ReLU activation function: 0.9984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data\n",
    "x_train = np.linspace(-10,10,100).reshape(-1, 1)\n",
    "y_train = (2.0 + 5 * x_train + 0.1 * x_train**2).reshape(-1,1)\n",
    "\n",
    "\n",
    "# Scales the data using StandardScaler from Scikit-learn\n",
    "scaler_y = StandardScaler()\n",
    "scaler_x= StandardScaler()\n",
    "\n",
    "# Standardiser x_train og y_train\n",
    "x_train_scaled = scaler_x.fit_transform(x_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train_scaled, y_train_scaled, test_size=0.2, random_state=20)\n",
    "\n",
    "\n",
    "# Defines the in- and output-size of the network\n",
    "network_input_size = 1 \n",
    "layer_output_sizes = [10,10, 1] #Two hidden layers with 10 nodes. \n",
    "\n",
    "# Creates the layers with the specified structure\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(10,10), activation=\"relu\", solver=\"adam\", learning_rate_init=0.001, max_iter=600, tol=1e-6, random_state=20, early_stopping=True)\n",
    "mlp_regressor.fit(x_train_split, y_train_split.ravel())\n",
    "mlp_pred_scaled = mlp_regressor.predict(x_test_split)  #Predict on the test data\n",
    "mlp_pred = scaler_y.inverse_transform(mlp_pred_scaled.reshape(-1,1))\n",
    "y_test_inverse = scaler_y.inverse_transform(y_test_split)\n",
    "\n",
    "mse_mlp = mse(y_test_inverse, mlp_pred)\n",
    "print( f\"MSE using Scikit-learn:{mse_mlp:.4f}\")\n",
    "print( f\"MSE own NN using ReLU activation function:{mse_ReLU:.4f}\")\n",
    "\n",
    "r2_mlp = R2(y_test_inverse, mlp_pred)\n",
    "print(f\"R² using Scikit-learn: {r2_mlp:.4f}\")\n",
    "print(f\"R² own NN using ReLU activation function: {r2_ReLU:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
