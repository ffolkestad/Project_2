{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This notebook consist of the own Feed Fordward Neural Network. This code dervies from the weekly exercise from week 42, and lecture notes from week 38, 39, 40 and 41.\"\n",
    "\n",
    "# Importing necesscary libaries\n",
    "import autograd.numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This section loads the data, splits it into training and test-dara, and scales it using Scikit-learn StandardScalar\"\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# This adds a cloum for the bias\n",
    "X_train_scaled = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_test_scaled = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Defines the binary cross-entropy cost\n",
    "def binary_cross_entropy(theta, X, y, lambda_reg=0.01):\n",
    "    n = len(y)\n",
    "    predictions = sigmoid(X @ theta)\n",
    "    cost = -(1/n) * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    l2_penalty = lambda_reg * np.sum(theta[1:]**2)  # to avoid that the bias term gets regulated\n",
    "    return cost + l2_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Autograd to calculate the gradients\n",
    "gradient_func = grad(binary_cross_entropy)\n",
    "\n",
    "# Defines the function for the logistic regression with SGD\n",
    "def logistic_regression_sgd(X, y, n_epochs=500, batch_size=16, lambda_reg=0.01, momentum=0.9, delta=1e-6):\n",
    "    m, n= X.shape\n",
    "    theta = np.random.randn(n)  # Initialize weights randomly\n",
    "    t0, t1 = 5, 50  # Parameters for learning schedule\n",
    "\n",
    "    change = np.zeros_like(theta)\n",
    "    Giter = np.zeros_like(theta)\n",
    "    def learning_schedule(t):\n",
    "        return t0 / (t + t1)\n",
    "\n",
    "    # Run mini-batch SGD, from weekly exercise week 41\n",
    "    for epoch in range(n_epochs):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_shuffled[i:i + batch_size]\n",
    "            yi = y_shuffled[i:i + batch_size]\n",
    "            gradients = gradient_func(theta, xi, yi, lambda_reg)\n",
    "\n",
    "            Giter += gradients*gradients\n",
    "            adjusted_gradients = gradients/(delta + np.sqrt(Giter))\n",
    "            eta = learning_schedule(epoch *(m//batch_size)+i//batch_size)\n",
    "\n",
    "            new_change = momentum*change - eta*adjusted_gradients\n",
    "            theta += new_change\n",
    "            change = new_change\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Using the sigmoid function to return the probability\n",
    "def predict_logistic_regression(X, theta):\n",
    "    y_pred_proba = 1/ (1+np.exp(-X @ theta))\n",
    "    return y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9736842105263158\n",
      "Test loss 0.1072396307747428\n"
     ]
    }
   ],
   "source": [
    "# Calculate teta, probability score, accuracy ans loss. \n",
    "theta_sgd = logistic_regression_sgd(X_train_scaled, y_train, n_epochs=500, batch_size=16, lambda_reg=0.01)\n",
    "y_test_prob = predict_logistic_regression(X_test_scaled, theta_sgd)\n",
    "pred_sgd =(y_test_prob >=0.5).astype(int)\n",
    "\n",
    "accuracy_sgd = accuracy_score(y_test, pred_sgd)\n",
    "print(f\"Accuracy {accuracy_sgd}\")\n",
    "test_loss_sgd = log_loss(y_test,y_test_prob)\n",
    "print(f\"Test loss {test_loss_sgd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn Logistic Regression Accuracy: 0.9737\n",
      "Scikit-Learn Logistic test loss: 0.0718\n"
     ]
    }
   ],
   "source": [
    "\"The main part of this code dervie from lecture notes week 38.\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_reg = LogisticRegression(penalty=\"l2\",random_state=1, max_iter=500)  \n",
    "\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "proba_sklearn = log_reg.predict_proba(X_test_scaled)[:,1]\n",
    "pred_sklearn = (proba_sklearn >=0.5).astype(int)\n",
    "\n",
    "accuracy_sklearn = accuracy_score(y_test, pred_sklearn)\n",
    "test_loss_sklearn = log_loss(y_test, proba_sklearn)\n",
    "\n",
    "print(f\"Scikit-Learn Logistic Regression Accuracy: {accuracy_sklearn:.4f}\")\n",
    "print(f\"Scikit-Learn Logistic test loss: {test_loss_sklearn:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
